# -*- coding: utf-8 -*-
"""
Created on Sun Mar  8 20:43:28 2020

@author: emet
"""

import pandas as pd
import re
finalResult = {
        'no_doi': [],#list of citations
        'doi_does_not_return_paper': [],#list of citations
        'paper_found': []#list of dictionaries (see below)
}
                                

#Example of one of the dicts that would go in the "paper_found" list:
def makeBlankDict():
    myDict = {
        'title':'',
        'num_authors':'',
        'authors':[
                        {
                        'name':'Sample McExample',
                        'gender':'',
                        'institution':'',
                        'country':'',
                        'rank':''
                        }
                    ],
        'origonal_citation':'',
        'date_published':'',
        'publisher':''
    }
    myDict['authors'].pop(0)
    return myDict


# FORMATTING DATA==============================================================

df = pd.read_csv(r'C:\Users\emet\Desktop\COMP\Freelon\journal_articles_1.csv')
list_of_citations = []
no_doi = []
dois =[]

#iterate over data frame. Seperate it into one list of DOIS and one list of citations with no DOI
for i in range(len(df)):
    
    citation = df.iloc[i,0]
    list_of_citations.append(citation)
    
    doi = re.findall("DOI 10(.*)", citation)
    
    if (len(doi) > 0):
        dois.append("10." + doi[0])

    else:
        no_doi.append(citation)

def findCitation(doi):
    for cit in list_of_citations:
        if doi.strip('https://doi.org/10.') in cit:
            return cit

# MAKING THE SOUP==============================================================     
import requests
from bs4 import BeautifulSoup

headers = requests.utils.default_headers()

user_agent_list = [
   #Chrome
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    #Firefox
    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',
    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'
]


def searchString(tagType, searchTerm, soups): #tag type can be "any"
    myList = []
    if tagType == "any":
        for soup in soups:
            myList.append(soup.find_all(text = re.compile(searchTerm)))
    else:
        for soup in soups:
            myList.append(soup.find_all(tagType, text = re.compile(searchTerm)))
    
    return myList

def searchClass(tagType, myClass, soups):
    myList = []
    for soup in soups:
        myList.append(soup.find_all(tagType, {'class':  myClass}))
    return myList

def findRank(author):
    #TODO
    return 'captain'

def guessGender(author):
    #TODO 
    return 'werewolf'
    
def findCountry(uni):
    #TODO 
    return 'neverland'

def extractInfo(soups,doi): 
   myDict = makeBlankDict()
   
   if (len(searchClass("div","SAGECopyright",soups)[1]) > 0):
       
       title_div = str(searchString('title', '.',soups)[0][0])
       myDict['title'] = re.findall("<title>(.)* - ", title_div)
       
       authors = re.findall("[-,] ([a-zA-Z][^,]*)", title_div)
       myDict['num_authors'] = len(authors)
       
       auth_institutions = []
       for div in searchClass('div', 'artice-info-affiliation', soups).pop():
           uni = re.findall('">([A-Za-z ]*)<\/div',str(div.text))
           auth_institutions.append(uni)
           
       i = 0
       while (i < myDict['num_authors']):
           myDict['authors'].append({
                   'name':authors[i],
                   'gender':guessGender(authors[i]),
                   'institution': "author location not on page" if (len(auth_institutions) <= i) else auth_institutions[i],
                   'country': "author location not on page" if (len(auth_institutions) <= i) else findCountry(auth_institutions[i]),
                   'rank':findRank(authors[i])
                   })
           i += 1
           
       myDict['origonal_citation'] = findCitation(doi)
       myDict['date_published'] = re.findall('<\/b> ([a-zA-Z 0-9,]*)',str(searchClass('span','publicationContentEpubDate',soups)[1]))[0]
       myDict['publisher'] = 'Sage Journals';
       
       return ('SAGE',myDict)
       
   elif(len(searchString('any', 'DOI cannot be found in the DOI System', soups)[0]) > 0):
       return ('ERROR',{})
   
   else:
       return ('dunno',{})
    
# SEARCHING DATA===============================================================
import random
import time
import sys

dois = ["https://doi.org/"+doi if doi != "" else doi for doi in dois]
doi_does_not_return_paper = []

mystery_papers = []
sage_papers = []

numSage = 0
numPapersDone = 0

for doi in dois:
    if doi != "":
      
        numPapersDone += 1
        print(numPapersDone)
        
        e = ''
        try: 
            headers.update({ 'User-Agent': random.choice(user_agent_list)}) #switch browsers to keep em guessing
            req = requests.get(doi, headers)
        except:
            e = sys.exc_info()[0] #the error
            doi_does_not_return_paper.append((doi,e))
            print("paper appended to 'doi_does_not_return_paper' (threw exception)")
        
        if (e == ''):
            soup0 = BeautifulSoup(req.content, 'xml')
            soup1 = BeautifulSoup(req.content, 'html.parser')
            soup2 = BeautifulSoup(req.content, 'lxml')
            
            soups = [soup0,soup1,soup2] #in case I need to add more parsers
            
            info = extractInfo(soups,doi) #extract info returns a tuple with a dict and a publisher name
            
            if (info[0] == 'SAGE'):
                numSage += 1
                sage_papers.append((doi, info[1]))
                print("paper appended to 'sage_papers'")
            elif (info[0] == 'ERROR'):
                doi_does_not_return_paper.append((doi,'no error thrown'))
                print("paper appended to 'doi_does_not_return_paper' (did not throw exception)")
            else: 
                mystery_papers.append((doi, info[1]))
                print("paper appended to 'mystery_papers'")
                
            
        time.sleep(.5)
        
        #if (len(sage_papers) >= 2):
            #break

# Try All Possible Period Iterations===========================================

def ppis1(index1, myStr): #put periods into string
    result = myStr[:index1] + '.' + myStr[index1:]
    return result

def ppis2(index1, index2, myStr): #put periods into string
    result = myStr[:index1] + '.' + myStr[index1:]

    if index1 <= index2:
        index2 += 1
        
    result = result[:index2] + '.' + result[index2:]
    return result

def ppis3(index1, index2, index3, myStr): #put periods into string #TODO
    result = myStr[:index1] + '.' + myStr[index1:]

    if index1 <= index2:
        index2 += 1
        
    result = result[:index2] + '.' + result[index2:]
    
    if index1 <= index3:
        index3 += 1
    if index2 <= index3:
        index3 += 1
        
    result = result[:index3] + '.' + result[index3:]
    
    return result

def pp1(myStr): #period possibilities
    myList = []
    i1 = 0
    while i1 + 1 < len(myStr):
        i1 += 1
        myList.append(ppis1(i1, myStr))
    return myList
    
def pp2(myStr):
    myList = []
    i1 = 0
    while i1 + 1 < len(myStr):
        i1 += 1
        i2 = i1
        while i2 + 1 < len(myStr):
            i2 += 1
            myList.append(ppis2(i1, i2, myStr));
    return myList

def pp3(myStr):#this could definitely be done with recursion
    myList = []
    i1 = 0
    while i1 + 1 < len(myStr):
        i1 += 1
        i2 = i1
        while i2 + 1 < len(myStr):
            i2 += 1
            i3 = i2
            while i3 + 1 < len(myStr):
                i3 += 1
                print(ppis3(i1, i2, i3, myStr))
                myList.append(ppis3(i1, i2, i3, myStr));
    return myList

listOfDois = []
for elem in doi_does_not_return_paper:
    if elem[1] == 'no error thrown':
        myDoi = elem[0].strip('https://doi.org/10.').replace(".","")
        iterations = []
        iterations.extend(pp1(myDoi))
        iterations.extend(pp2(myDoi))
        iterations.extend(pp3(myDoi))
        listOfDois.append((myDoi, iterations))

numPapersDone = 0
exceptions = []
doisThatWork = {}
for elem in listOfDois:
    for iteration in elem[1]:
        doi = 'https://doi.org/10.'+iteration
        print(doi)
        e = ''
        try: 
            headers.update({ 'User-Agent': random.choice(user_agent_list)}) #switch browsers to keep em guessing
            req = requests.get(doi, headers)
        except:
            e = sys.exc_info()[0] #the error
            exceptions.append((doi,e))
        
        if (e == ''):
            soup0 = BeautifulSoup(req.content, 'xml')
            soup1 = BeautifulSoup(req.content, 'html.parser')
            soup2 = BeautifulSoup(req.content, 'lxml')
            
            soups = [soup0,soup1,soup2] #in case I need to add more parsers
            
            info = extractInfo(soups,doi) #extract info returns a tuple with a dict and a publisher name
            
            if (info[0] != 'ERROR'):
                numPapersDone += 1000000
                doisThatWork[elem[0]] = iteration
            
        time.sleep(.1)
        
        numPapersDone += 1
        print(numPapersDone)
        
        if (len(doisThatWork) > 0):
            print(doisThatWork)
        else:
            print("no luck")
    
